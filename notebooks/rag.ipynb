{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40762200-a8a0-40b1-8830-7270bb4135f6",
   "metadata": {},
   "source": [
    "from example https://github.com/reichenbch/RAG-examples/blob/main/LangChain%20LLamaIndex%20RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5a7b05-4447-41d3-80f8-595a7ec06a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/sulcan/Documents/ipac-logbook/code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a4aad6-24b7-44e2-8a97-4195e0e41dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b5888bf75240288e17df5862bf7d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from hugginface_wrapper import HuggingFaceLLM\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # 'microsoft/phi-1.5'\n",
    "device = 'cuda'\n",
    "quantization_config = None # BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = model_id, \n",
    "                                             device_map = device,\n",
    "                                             quantization_config = quantization_config)\n",
    "\n",
    "df = pd.read_csv('/home/sulcan/Documents/malapa/data/abstracts.csv')\n",
    "abstracts = list(df['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca08af4-8dcb-44dc-8946-e8d8480a299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(model_data = {'tokenizer' : tokenizer, 'model' : model}, max_new_tokens = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3acb95d-3a3d-4aa9-b887-0ab12f56b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as a question answering bot which uses the context mentioned and answer in\n",
      "a concise manner and doesn't make stuff up.\n",
      "You will answer question based on the context - {context}. \n",
      "You will create content in English language.\n",
      "Question: {question}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "language = \"English\"\n",
    "context = \"{context}\"\n",
    "\n",
    "template =  \"\"\"I want you to act as a question answering bot which uses the context mentioned and answer in\n",
    "a concise manner and doesn't make stuff up.\n",
    "You will answer question based on the context - \"\"\" + str(context) + \"\"\". \n",
    "You will create content in \"\"\" + str(language) + \"\"\" language.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "print(template)\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables = ['question', 'context'], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0299fc7-b50b-45c9-8561-ff6e98338d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") \n",
    "vector_store = FAISS.from_texts(abstracts, embeddings)\n",
    "vector_store.save_local(\"../data/faiss_doc_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1680768a-16d0-4710-95cf-78f05103584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318f6600-d64c-446f-a116-aa572c20ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever(), chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e91d8e-83a7-4681-b688-eb9d0398467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sulcan/.local/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"Give me a list of submissions that talk about thoughts.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970adf15-cc23-4d92-8b67-a632fa4b45fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as a question answering bot which uses the context mentioned and answer in\n",
      "a concise manner and doesn't make stuff up.\n",
      "You will answer question based on the context - In this paper, we show a textual analysis of past ICALEPCS and IPAC conference proceedings to gain insights into the research trends and topics discussed in the field. We use natural language processing techniques to extract meaningful information from the abstracts and papers of past conference proceedings. We extract topics to visualize and identify trends, analyze their evolution to identify emerging research directions, and highlight interesting publications based solely on their content with an analysis of their network. Additionally, we will provide an advanced search tool to better search the existing papers to prevent duplication and easier reference findings. Our analysis provides a comprehensive overview of the research landscape in the field and helps researchers and practitioners to better understand the state-of-the-art and identify areas for future research.\n",
      "\n",
      "Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions. These intricate devices challenge comprehension, even within individual facilities.    To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books.    We automated data collection and question generation to minimize expert involvement and make the data publicly available.    PACuna demonstrates proficiency in addressing accelerator questions, validated by experts.    Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some specific questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities.\n",
      "\n",
      "The electronic logbook (elog) system used at Brookhaven National Laboratoryâ€™s Collider-Accelerator Department (C-AD) allows users to customize logbook settings, including specification of favorite logbooks. Using machine learning techniques, configurations can be further personalized to provide users with a view of entries that match their specific interests. Natural language processing (NLP) models are used to augment the elog system by classifying and finding similarities in entries. A command line interface tool is used to ease automation of NLP tasks in the controls system. A test web interface will be developed for users to enter phrases, terms, and sentences as search terms for the NLP models. The website will return useful information about a given search term. This technique will create recommendations for each user, filtering out unnecessary results generated by current search techniques.\n",
      "\n",
      "Logbooks store important knowledge of activities and events that occur during accelerator operations. However, orientation and automation of accelerator logbooks can be challenging due to various challenges like very technical texts or content being hidden in images instead of text. As AI technologies like natural language processing continue to mature, they present opportunities to address these challenges in the context of particle accelerator logbooks.    In this work, we explore the potential use of state-of-the-art AI techniques in particle accelerator logbooks. Our goals are to help operators increase the FAIR-ness (findability, accessibility, interoperability, reusability) of logbooks by exploiting the multimodal information to make everyday use easier with multimodal LLM (large language models).. \n",
      "You will create content in English language.\n",
      "Question: Give me a list of submissions that talk about thoughts.\n",
      "Answer: Based on the context, the paper discusses research trends and topics in the field of particle accelerators by analyzing past conference proceedings using natural language processing techniques. It does not directly mention a list of submissions that talk about thoughts. However, the paper mentions the use of natural language processing to extract meaningful information from abstracts and papers, which could potentially include submissions about thoughts or related topics. To obtain an accurate list, it would be necessary to perform a specific search using the advanced search tool mentioned in the paper or by manually filtering the results based on the abstracts or keywords.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864b221-a97c-459e-a13d-e45faebcc72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3f73e-2c80-4efb-9f9e-635d287bb988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
