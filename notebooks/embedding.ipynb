{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563bed01-8ffe-46c2-a095-890be6fa7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers.util import cos_sim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle, gzip\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sys.path.insert(0,'/home/sulcan/Documents/ipac-logbook/code/')\n",
    "from mmd import *\n",
    "\n",
    "i = '_min_max_uncased_'\n",
    "\n",
    "data_folder = '/home/sulcan/Documents/ipac-logbook/data/data_acc/'\n",
    "model_folder = f'/home/sulcan/Documents/ipac-logbook/models/simcse/{i}'\n",
    "max_seq_length = 512\n",
    "min_seq_length = 16\n",
    "uncase = True\n",
    "device = 'cuda:0'\n",
    "\n",
    "if uncase:\n",
    "    model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "else:\n",
    "    model_name = \"allenai/scibert_scivocab_cased\"\n",
    "    \n",
    "folders = [f'{data_folder}/arxiv/',\\\n",
    "           f'{data_folder}/jacow/',\\\n",
    "           f'{data_folder}/books/']\n",
    "\n",
    "files = []\n",
    "for folder in folders:\n",
    "    files.extend(glob(folder + '*.mmd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d590e1f8-4141-422e-8199-a5654e8ef167",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b54d32-5856-4e3a-96b1-a84ed7c8b812",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9d7d9",
   "metadata": {},
   "source": [
    "Opening mmd files, filtering equations, and chining sentences (sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b6e186-99e1-4663-aa61-2000c463d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # loading data\n",
    "    print('loading data...')\n",
    "    data_mmd = {}\n",
    "    for file in tqdm(sorted(files)):\n",
    "        with open(file, 'r') as f:\n",
    "            data_mmd[file] = f.read()\n",
    "    print('... data loaded')\n",
    "    \n",
    "    print('preparing data ...')\n",
    "    # preparing equations and removing tables\n",
    "    data_mmd = prepare_mmd_eqations_and_tables_for_simcse(data_mmd)\n",
    "    print('... data prepared')\n",
    "    \n",
    "    print('chunking sentences ...')\n",
    "    # chunking sentences\n",
    "    train_sentences = []\n",
    "    for k in tqdm(data_mmd):\n",
    "        for par in data_mmd[k].split('\\n\\n'):\n",
    "            par = re.sub('#+',' ',par)\n",
    "            par = re.sub('\\s+',' ', par)\n",
    "            train_sentences.extend(sent_tokenize(par))\n",
    "    \n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','wb') as f:\n",
    "        pickle.dump({'data_mmd' : data_mmd, 'train_sentences' : train_sentences}, f)\n",
    "    print('...sentences chunked.')\n",
    "    \n",
    "else:\n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','rb') as f:\n",
    "        data = pickle.load(f)# \n",
    "        data_mmd = data['data_mmd']\n",
    "        train_sentences = data['train_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32db490",
   "metadata": {},
   "source": [
    "Filtering too long or too short tokens (short usually don't contain anything informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a737ec-f5ba-4107-a644-a13414720968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7809227/7809227 [12:40<00:00, 10267.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_filtered = []\n",
    "\n",
    "for i in tqdm(range(len(train_sentences))):\n",
    "    sent = train_sentences[i]\n",
    "    length = len(model.tokenizer.encode(sent))\n",
    "    if length < max_seq_length and length > min_seq_length:\n",
    "        if uncase:\n",
    "            sent = sent.lower()\n",
    "            \n",
    "        train_sentences_filtered.append(sent)\n",
    "train_sentences = train_sentences_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_sentences)\n",
    "# import pickle\n",
    "# with open('/home/sulcan/train_sentences.pickle','rb') as f:\n",
    "#     train_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5361e6-3179-4aba-83f7-1afed6bc3970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4989936/4989936 [00:22<00:00, 224308.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5fac315d814c638c35ce920afbfe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f85705d1e7040b1a48058c5e62c6dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66df43b488504581a15cc1d9c0c7d79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a5f1d6744d41569474b274c5e38e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert train sentences to sentence pairs\n",
    "train_data = [InputExample(texts=[s, s]) for s in tqdm(train_sentences)]\n",
    "\n",
    "# DataLoader to batch your data\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Call the fit method\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], epochs=3, show_progress_bar=True\n",
    ")\n",
    "\n",
    "# model.save(\"output/simcse-model\")\n",
    "if True:\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befb0ba",
   "metadata": {},
   "source": [
    "### Testing / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bceba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_folder, max_seq_length=128)\n",
    "ooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model1 = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b651246-d094-4304-9bcc-fc3a038be767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# model2 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model2 = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62367f3f-1987-4b76-a6ff-42c0a4a50bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(model, sents):\n",
    "    e = model.encode(sents)\n",
    "    return cos_sim(e,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e2e5ac-6d3e-470f-9707-66fc011d4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "tensor([[1.0000, 0.1714, 0.1250],\n",
      "        [0.1714, 1.0000, 0.6175],\n",
      "        [0.1250, 0.6175, 1.0000]])\n",
      "their\n",
      "tensor([[1.0000, 0.4815, 0.7118],\n",
      "        [0.4815, 1.0000, 0.6446],\n",
      "        [0.7118, 0.6446, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = ['ouch, I have a cavity in my tooth', 'superconducting cavity', 'cavity detuned']\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2ece03b-7a84-43b1-8822-da9443c18abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "tensor([[1.0000, 0.2472, 0.1704],\n",
      "        [0.2472, 1.0000, 0.1264],\n",
      "        [0.1704, 0.1264, 1.0000]])\n",
      "their\n",
      "tensor([[1.0000, 0.5968, 0.5626],\n",
      "        [0.5968, 1.0000, 0.5414],\n",
      "        [0.5626, 0.5414, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = ['radiation', 'synchroton', 'problem']\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06fd43f2-6eed-4fdb-a6f1-abbfd9b84792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "tensor([[1.0000, 0.3202, 0.1785, 0.1457, 0.1953],\n",
      "        [0.3202, 1.0000, 0.4263, 0.1973, 0.3299],\n",
      "        [0.1785, 0.4263, 1.0000, 0.3675, 0.4771],\n",
      "        [0.1457, 0.1973, 0.3675, 1.0000, 0.4892],\n",
      "        [0.1953, 0.3299, 0.4771, 0.4892, 1.0000]])\n",
      "their\n",
      "tensor([[1.0000, 0.6057, 0.5634, 0.6982, 0.6396],\n",
      "        [0.6057, 1.0000, 0.8295, 0.5831, 0.6187],\n",
      "        [0.5634, 0.8295, 1.0000, 0.5835, 0.6225],\n",
      "        [0.6982, 0.5831, 0.5835, 1.0000, 0.6210],\n",
      "        [0.6396, 0.6187, 0.6225, 0.6210, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Accelerating metal tube making electrons running faster?',\n",
    "    'Superconducting cavity',\n",
    "    'RF Cavity',\n",
    "    'Accelerator',\n",
    "    'Beam Tube']\n",
    "\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c309af4-2f92-4c39-99c5-5abcf44da891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "tensor([[1.0000, 0.8607, 0.4715, 0.4525, 0.3635, 0.3478],\n",
      "        [0.8607, 1.0000, 0.3727, 0.3972, 0.3129, 0.3295],\n",
      "        [0.4715, 0.3727, 1.0000, 0.3588, 0.2108, 0.1905],\n",
      "        [0.4525, 0.3972, 0.3588, 1.0000, 0.1495, 0.2868],\n",
      "        [0.3635, 0.3129, 0.2108, 0.1495, 1.0000, 0.2724],\n",
      "        [0.3478, 0.3295, 0.1905, 0.2868, 0.2724, 1.0000]])\n",
      "their\n",
      "tensor([[1.0000, 0.6652, 0.3914, 0.4542, 0.4560, 0.4527],\n",
      "        [0.6652, 1.0000, 0.4791, 0.5385, 0.4785, 0.5301],\n",
      "        [0.3914, 0.4791, 1.0000, 0.5250, 0.5296, 0.6150],\n",
      "        [0.4542, 0.5385, 0.5250, 1.0000, 0.5355, 0.5744],\n",
      "        [0.4560, 0.4785, 0.5296, 0.5355, 1.0000, 0.6121],\n",
      "        [0.4527, 0.5301, 0.6150, 0.5744, 0.6121, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'BESSY',\n",
    "    'DESY',\n",
    "    'European XFEL',\n",
    "    'PETRA'\n",
    "    'HZB',\n",
    "    'synchroton',\n",
    "    'linac']\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56266c64-92a0-41b4-b723-b46b62b1d39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
