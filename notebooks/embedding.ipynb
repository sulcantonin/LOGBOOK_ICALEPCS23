{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563bed01-8ffe-46c2-a095-890be6fa7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers.util import cos_sim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle, gzip\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sys.path.insert(0,'/home/sulcan/Documents/ipac-logbook/code/')\n",
    "from mmd import *\n",
    "\n",
    "i = '_min_max_uncased_'\n",
    "\n",
    "data_folder = '/home/sulcan/Documents/ipac-logbook/data/data_acc/'\n",
    "model_folder = f'/home/sulcan/Documents/ipac-logbook/models/simcse/{i}'\n",
    "max_seq_length = 512\n",
    "min_seq_length = 16\n",
    "uncase = True\n",
    "device = 'cuda:1'\n",
    "\n",
    "if uncase:\n",
    "    model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "else:\n",
    "    model_name = \"allenai/scibert_scivocab_cased\"\n",
    "    \n",
    "folders = [f'{data_folder}/arxiv/',\\\n",
    "           f'{data_folder}/jacow/',\\\n",
    "           f'{data_folder}/books/']\n",
    "\n",
    "files = []\n",
    "for folder in folders:\n",
    "    files.extend(glob(folder + '*.mmd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d590e1f8-4141-422e-8199-a5654e8ef167",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b54d32-5856-4e3a-96b1-a84ed7c8b812",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9d7d9",
   "metadata": {},
   "source": [
    "Opening mmd files, filtering equations, and chining sentences (sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b6e186-99e1-4663-aa61-2000c463d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # loading data\n",
    "    print('loading data...')\n",
    "    data_mmd = {}\n",
    "    for file in tqdm(sorted(files)):\n",
    "        with open(file, 'r') as f:\n",
    "            data_mmd[file] = f.read()\n",
    "    print('... data loaded')\n",
    "    \n",
    "    print('preparing data ...')\n",
    "    # preparing equations and removing tables\n",
    "    data_mmd = prepare_mmd_eqations_and_tables_for_simcse(data_mmd)\n",
    "    print('... data prepared')\n",
    "    \n",
    "    print('chunking sentences ...')\n",
    "    # chunking sentences\n",
    "    train_sentences = []\n",
    "    for k in tqdm(data_mmd):\n",
    "        for par in data_mmd[k].split('\\n\\n'):\n",
    "            par = re.sub('#+',' ',par)\n",
    "            par = re.sub('\\s+',' ', par)\n",
    "            train_sentences.extend(sent_tokenize(par))\n",
    "    \n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','wb') as f:\n",
    "        pickle.dump({'data_mmd' : data_mmd, 'train_sentences' : train_sentences}, f)\n",
    "    print('...sentences chunked.')\n",
    "    \n",
    "else:\n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','rb') as f:\n",
    "        data = pickle.load(f)# \n",
    "        data_mmd = data['data_mmd']\n",
    "        train_sentences = data['train_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32db490",
   "metadata": {},
   "source": [
    "Filtering too long or too short tokens (short usually don't contain anything informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a737ec-f5ba-4107-a644-a13414720968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7809227/7809227 [12:40<00:00, 10267.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_filtered = []\n",
    "\n",
    "for i in tqdm(range(len(train_sentences))):\n",
    "    sent = train_sentences[i]\n",
    "    length = len(model.tokenizer.encode(sent))\n",
    "    if length < max_seq_length and length > min_seq_length:\n",
    "        if uncase:\n",
    "            sent = sent.lower()\n",
    "            \n",
    "        train_sentences_filtered.append(sent)\n",
    "train_sentences = train_sentences_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_sentences)\n",
    "# import pickle\n",
    "# with open('/home/sulcan/train_sentences.pickle','rb') as f:\n",
    "#     train_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5361e6-3179-4aba-83f7-1afed6bc3970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4989936/4989936 [00:22<00:00, 224308.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5fac315d814c638c35ce920afbfe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f85705d1e7040b1a48058c5e62c6dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66df43b488504581a15cc1d9c0c7d79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a5f1d6744d41569474b274c5e38e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/311871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert train sentences to sentence pairs\n",
    "train_data = [InputExample(texts=[s, s]) for s in tqdm(train_sentences)]\n",
    "\n",
    "# DataLoader to batch your data\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Call the fit method\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], epochs=3, show_progress_bar=True\n",
    ")\n",
    "\n",
    "# model.save(\"output/simcse-model\")\n",
    "if True:\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befb0ba",
   "metadata": {},
   "source": [
    "### Testing / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bceba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    word_embedding_model = models.Transformer(model_folder, max_seq_length=128)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b651246-d094-4304-9bcc-fc3a038be767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model2 = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62367f3f-1987-4b76-a6ff-42c0a4a50bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(model, sentences):\n",
    "    e = model.encode(sentences)\n",
    "    return cos_sim(e,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ece03b-7a84-43b1-8822-da9443c18abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16e2e5ac-6d3e-470f-9707-66fc011d4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.3864, 0.6862],\n",
      "        [0.3864, 1.0000, 0.4369],\n",
      "        [0.6862, 0.4369, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(_eval(model2,['ouch, I have a cavity in my tooth', 'superconducting cavity', 'cavity detuned']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
