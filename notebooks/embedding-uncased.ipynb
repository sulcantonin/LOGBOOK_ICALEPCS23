{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563bed01-8ffe-46c2-a095-890be6fa7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers.util import cos_sim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle, gzip\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle, gzip\n",
    "from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,'/home/sulcan/Documents/ipac-logbook/code/')\n",
    "from mmd import *\n",
    "gd = lambda x,i : x[list(x.keys())[i]]\n",
    "\n",
    "max_seq_length = 512\n",
    "min_seq_length = 16\n",
    "uncase = True\n",
    "device = 'cuda'\n",
    "epochs = 8\n",
    "\n",
    "name = f'_min{min_seq_length}_max{max_seq_length}_' + ('un' if uncase else '') + f'cased_plain_scibert-eqplaceholder_{epochs}epochs_'\n",
    "data_folder = '/home/sulcan/Documents/ipac-logbook/data/data_acc/'\n",
    "model_folder = f'/home/sulcan/Documents/ipac-logbook/models/simcse/{name}'\n",
    "\n",
    "\n",
    "if uncase:\n",
    "    model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "    # model_name = 'distilbert/distilbert-base-uncased'\n",
    "    # model_name = 'princeton-nlp/sup-simcse-roberta-base'\n",
    "else:\n",
    "    model_name = \"allenai/scibert_scivocab_cased\"\n",
    "    # model_name = 'distilbert/distilbert-base-cased'\n",
    "    # model_name = 'princeton-nlp/sup-simcse-roberta-base'\n",
    "    \n",
    "folders = [f'{data_folder}/arxiv/',\\\n",
    "           f'{data_folder}/jacow/',\\\n",
    "           f'{data_folder}/books/',\\\n",
    "    ]\n",
    "\n",
    "files = []\n",
    "for folder in folders:\n",
    "    files.extend(glob(folder + '*.mmd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d590e1f8-4141-422e-8199-a5654e8ef167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allenai/scibert_scivocab_uncased\n",
      "[CLS] hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length, do_lower_case = uncase)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = device)\n",
    "print(model_name)\n",
    "print(model.tokenizer.decode(model.tokenizer.encode('Hello World')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b54d32-5856-4e3a-96b1-a84ed7c8b812",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9d7d9",
   "metadata": {},
   "source": [
    "Opening mmd files, filtering equations, and chining sentences (sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b6e186-99e1-4663-aa61-2000c463d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48113/48113 [00:37<00:00, 1291.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... data loaded\n",
      "preparing data (equations, tables)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48113/48113 [15:33<00:00, 51.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... data prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48113/48113 [29:37<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...sentences chunked.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # loading data\n",
    "    print('loading data...')\n",
    "    data_mmd = {}\n",
    "    for file in tqdm(sorted(files)):\n",
    "        with open(file, 'r') as f:\n",
    "            data_mmd[file] = f.read()\n",
    "    print('... data loaded')\n",
    "    \n",
    "    print('preparing data (equations, tables)...')\n",
    "    # preparing equations and removing tables\n",
    "    data_mmd = prepare_mmd_eqations_and_tables_for_simcse(data_mmd)\n",
    "    print('... data prepared')\n",
    "    \n",
    "    \n",
    "    # splitting into smaller chunks of overlapping text\n",
    "    train_sentences = []\n",
    "    splitter = SentenceTransformersTokenTextSplitter.from_huggingface_tokenizer(\n",
    "        model.tokenizer, chunk_size=max_seq_length, chunk_overlap=0)\n",
    "    \n",
    "    for k in tqdm(data_mmd):\n",
    "        document = data_mmd[k]\n",
    "        document = re.sub('#+',' ',document)\n",
    "        document = re.sub('\\s+',' ', document)\n",
    "        train_sentences.extend(splitter.split_text(document))\n",
    "    \n",
    "    # chunking by sentences, trains the model to be quite biased towards rather shorter sentences\n",
    "    '''\n",
    "    print('chunking paragraphs into sentences ...')\n",
    "    # chunking sentences\n",
    "    train_sentences = []\n",
    "    for k in tqdm(data_mmd):\n",
    "        for par in data_mmd[k].split('\\n\\n'):\n",
    "            par = re.sub('#+',' ',par)\n",
    "            par = re.sub('\\s+',' ', par)\n",
    "            train_sentences.extend(sent_tokenize(par))\n",
    "            \n",
    "    train_sentences_filtered = []\n",
    "\n",
    "    for i in tqdm(range(len(train_sentences))):\n",
    "        sent = train_sentences[i]\n",
    "        length = len(model.tokenizer.encode(sent))\n",
    "        if length < max_seq_length and length > min_seq_length:            \n",
    "            train_sentences_filtered.append(sent)\n",
    "    train_sentences = train_sentences_filtered\n",
    "\n",
    "    with gzip.open(f'{data_folder}/train_sentences_{name}.pickle','wb') as f:\n",
    "         pickle.dump(train_sentences, f)\n",
    "    '''\n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','wb') as f:\n",
    "        pickle.dump({'data_mmd' : data_mmd, 'train_sentences' : train_sentences}, f)\n",
    "    print('...sentences chunked.')\n",
    "    \n",
    "else:\n",
    "    with open(f'{data_folder}/simcse_prepared_data.pickle.gzip','rb') as f:\n",
    "        data = pickle.load(f)# \n",
    "        data_mmd = data['data_mmd']\n",
    "        train_sentences = data['train_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    with gzip.open(f'{data_folder}/train_sentences_{name}.pickle','wb') as f:\n",
    "         pickle.dump(train_sentences, f)\n",
    "else:\n",
    "    with gzip.open(f'{data_folder}/train_sentences_{name}.pickle','rb') as f:\n",
    "             train_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2140992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. eoc can be successfully used in large hadron collider lhc as well as in a planned muon collider. figure the scheme of the eoc of a particle beam a and unwrapped optical scheme bthe eoc in the simpiest case of two dimensional cooling in the longitudinal and transverse xplanes is based on one pickup and one or more kicker undulators located at a distance determined by the betatron phase advance equation for first kicker undulator and equation for the next ones where equation... is the whole numbers. other elements of the cooling system are the optical amplifier typically optical parametric amplifier i. e. opa optical filters optical lenses movable screens and optical line with variable time delay see fig.. an optical delay line can be used together with or in some cases without isochronous passway between undulators to keep the phases of particles such that the kicker undulator decelerates the particles during the process of cooling.. to the foundations of enhanced optical cooling the total amount of energy carried out by undulator radiation ur emitted by electrons traversing an undulator according to classical electrodynamics is given by equation where equation is the classical electron radius equation equation are the electron charge and mass respectively equation is an averaged square of magnetic field along the undulator period equation equation is the relative velocity of the electron equation is the relativistic factor equation is the length of the undulator and equation is the number of undulator periods. for a planar harmonic undulator equation where equation is the peak of the undulator field. for a helical undulator equation. the spectral distribution of the first harmonic of ur for equation is given by equation where equation equation equation. filtered urws must be amplified and directed along the axis of the kicker undulator. if the density of energy in the urws has a gaussian distribution'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5361e6-3179-4aba-83f7-1afed6bc3970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 394600/394600 [00:01<00:00, 215624.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c86e0afdc048d2869367ae2905ca57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d8ffe91e4d44ffa41a91c47299fc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/24663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d9b5f1a2d242609bbb18b511cf6e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/24663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6285182790342d2af02c7b9096b5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/24663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99a85121e2e440f8a04826f45d9d681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/24663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b753523bacf406cac17800885f40974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/24663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert train sentences to sentence pairs\n",
    "train_data = [InputExample(texts=[s, s]) for s in tqdm(train_sentences)]\n",
    "\n",
    "# DataLoader to batch your data\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Call the fit method\n",
    "model.fit(\n",
    "     warmup_steps=0.0, #int(0.1 * len(train_sentences)),\n",
    "    # weight_decay = 0.0,\n",
    "    optimizer_params={'lr' : 2e-5},\n",
    "    train_objectives=[(train_dataloader, train_loss)], epochs=epochs, show_progress_bar=True\n",
    ")\n",
    "\n",
    "# model.save(\"output/simcse-model\")\n",
    "if True:\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08141b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befb0ba",
   "metadata": {},
   "source": [
    "### Testing / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer(model_folder)\n",
    "ooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model1 = SentenceTransformer(modules=[word_embedding_model, pooling_model], device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b651246-d094-4304-9bcc-fc3a038be767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model2 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# model2 = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62367f3f-1987-4b76-a6ff-42c0a4a50bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(model, sents):\n",
    "    e = model.encode(sents)\n",
    "    return cos_sim(e,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2e5ac-6d3e-470f-9707-66fc011d4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['ouch, I have a cavity in my tooth','rf cavity', 'superconducting cavity running on european xfel', 'i am tunning radio']\n",
    "print(sentences)\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ece03b-7a84-43b1-8822-da9443c18abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The radiation is really important in operating PETRA', \n",
    "             'PETRA is an important synchroton at DESY', \n",
    "             'I have many problems with exposure to radiation',\n",
    "             'Synchroton radiation is important for bremenstrallung']\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd43f2-6eed-4fdb-a6f1-abbfd9b84792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sentences1 = ['Motion control is assuming an increasingly pivotal role within modern large accelerator facilities, such as 4th generation storage ring-based light sources, SRF accelerators, and high-performance photon beamlines. For very high-Q SRF linacs, such as LCLS-II, the precise management of cavity resonance becomes indispensable for maintaining stable operations. Failing to do so would entail a significant upsurge in RF power requirements, consequently increasing operational and capital costs due to the necessity for additional RF power sources. We have developed an intelligent cavity resonance controller founded on a data-driven model, featuring an exceptionally lightweight surrogate mode engineered to address the intricate dynamics of forced cavities in the presence of microphonics and coupled with nonlinear Lorentz forces. The effectiveness of this mode has been rigorously validated through real SRF cavities at SLAC. We are currently in the process of implementing the controller on hardware, specifically the exiting LLRF system of LCLSII. Building on the success of this work, the model can be expanded to encompass general motion controls where exceptionally low-tolerance vibration is required. In this presentation, we will introduce the model and provide an overview of the latest test results.', 'During the operation of the Continuous Electron Beam Accelerator Facility (CEBAF), one or more unstable superconducting radio-frequency (SRF) cavities often cause beam loss trips while the unstable cavities themselves do not necessarily trip off. The present RF controls for the legacy cavities report at only 1 Hz, which is too slow to detect fast transient instabilities during these trip events. These challenges make the identification of an unstable cavity out of the hundreds installed at CEBAF a difficult and time-consuming task. To tackle these issues, a fast data acquisition system (DAQ) for the legacy SRF cavities has been developed, which records the sample at 5 kHz. A Principal Component Analysis (PCA) approach is being developed to identify anomalous SRF cavity behavior. We will discuss the present status of the DAQ system and PCA model, along with initial performance metrics. Overall, our method offers a practical solution for identifying unstable SRF cavities, contributing to increased beam availability and facility reliability.', 'Within the context of the European X-Ray Free-Electron Laser (EuXFEL), where 800 superconducting radio-frequency cavities (SRFCs) are employed to accelerate electron bunches to energies as high as 17.5 GeV, ensuring safe and optimal accelerator operation is crucial. In this work, we introduce a machine learning (ML)-enhanced approach for detecting anomalies, with a particular focus on identifying quenches, which can disrupt the superconductivity of the SRFCs, leading to operational interruptions. Our method consists of a two-stage analysis of the cavity dynamics. We first leverage analytical redundancy to process the data, and generate a residual for statistical testing and anomaly detection. Subsequently, we employ machine learning to distinguish quenching events from other anomalies. Different algorithms have been explored, and adapted in order to take into account the specificity of the data at hand. The evaluation, based on 2022 data, demonstrates the superior performance of our approach when compared to the currently deployed quench detection system.', 'In this study, we present a deep learning-based pipeline for predicting superconducting radio-frequency (SRF) cavity faults in the Continuous Electron Beam Accelerator Facility (CEBAF) at Jefferson Lab. We leverage pre-fault RF signals from C100-type cavities and employ deep learning to predict faults in advance of their onset. We train a binary classifier model to distinguish between stable and impending fault signals, where each cryomodule has a uniquely trained model. Test results show accuracies exceeding 99% in each of the six models for distinguishing between normal signals and pre-fault signals from a class of more slowly developing fault types, such as microphonics-induced faults. We describe results from a proof-of-principle demonstration on a realistic, imbalanced data set and report performance metrics. Encouraging results suggest that future SRF systems could leverage this framework and implement measures to mitigate the onset in more slowly developing fault types.']\n",
    "sentences1 = ['Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions. These intricate devices challenge comprehension, even within individual facilities.    To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books.    We automated data collection and question generation to minimize expert involvement and make the data publicly available.    PACuna demonstrates proficiency in addressing accelerator questions, validated by experts.    Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some specific questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities.', 'The Transformer is a deep learning architecture introduced in 2017, that has since then taken over the natural language processing field and has recently gained public popularity thanks to large language models like ChatGPT. The self-attention mechanism introduced with the Transformer allows it to learn complex patterns and relationships in data without explicitly using recurrent mechanisms like classic RNN-style architectures. While the Transformer was developed for sequence-to-sequence language modeling like translation tasks, the usefulness for time series prediction has been less explored in the machine learning community. Particularly, the lack of beginner-friendly tutorials and guides for using transformers with uni- and multivariate continuous input and outputs are not easily found online, as opposed to for natural language tasks. Therefore, this tutorial aims to introduce the Transformer architecture and how to use standard deep-learning library Transformer building blocks to construct a simple time series prediction model and explain the inputs and outputs of the transformer model. As an appendix, we will give a quick outlook of current state-of-the-art time series prediction architectures based on the basic Transformer as well as alternative modern time series forecasting methods.', 'The electronic logbook (elog) system used at Brookhaven National Laboratory’s Collider-Accelerator Department (C-AD) allows users to customize logbook settings, including specification of favorite logbooks. Using machine learning techniques, configurations can be further personalized to provide users with a view of entries that match their specific interests. Natural language processing (NLP) models are used to augment the elog system by classifying and finding similarities in entries. A command line interface tool is used to ease automation of NLP tasks in the controls system. A test web interface will be developed for users to enter phrases, terms, and sentences as search terms for the NLP models. The website will return useful information about a given search term. This technique will create recommendations for each user, filtering out unnecessary results generated by current search techniques.', 'Logbooks store important knowledge of activities and events that occur during accelerator operations. However, orientation and automation of accelerator logbooks can be challenging due to various challenges like very technical texts or content being hidden in images instead of text. As AI technologies like natural language processing continue to mature, they present opportunities to address these challenges in the context of particle accelerator logbooks.    In this work, we explore the potential use of state-of-the-art AI techniques in particle accelerator logbooks. Our goals are to help operators increase the FAIR-ness (findability, accessibility, interoperability, reusability) of logbooks by exploiting the multimodal information to make everyday use easier with multimodal LLM (large language models).']\n",
    "sentences2 = ['Reinforcement learning is a form of machine learning in which intelligent agents learn to solve complex problems by gaining experience. In current research, agents trained with reinforcement learning perform better than their human counterparts on problems that have historically been difficult for machines to solve. Particle accelerators are among the most advanced high-tech machines in the world. Modern scientific experiments place the highest demands on beam quality, making particle accelerator control extremely complex. Reinforcement learning is a promising avenue of research that has the potential to improve existing accelerator control solutions and enable new ones that have previously been impossible with conventional methods. The barrier of entry into reinforcement learning, however, is high and slows its adoption in the accelerator field. In this tutorial, we apply reinforcement learning to the task of tuning transverse beam parameters in a real-world accelerator beam line and focus in particular on solving the issues that arise in the context of particle accelerators, such as the high cost of samples, a large sim2real gap and the high non-linearity of the control and optimisation tasks under investigation.', 'Reinforcement Learning (RL) is a unique learning paradigm that is particularly well-suited to tackle complex control tasks, can deal with delayed consequences, and learns from experience without an explicit model of the dynamics of the problem. These properties make RL methods extremely promising for applications in particle accelerators, where the dynamically evolving conditions of both the particle beam and the accelerator systems must be constantly considered.    While the time to work on RL is now particularly favourable thanks to the availability of high-level programming libraries and resources, its implementation in particle accelerators is not trivial and requires further consideration.    In this context, the Reinforcement Learning for Autonomous Accelerators (RL4AA) international collaboration was established to consolidate existing knowledge, share experiences and ideas, and collaborate on accelerator-specific solutions that leverage recent advances in RL.    The collaboration was launched in February 2023 during the RL4AA’23 workshop at the Karlsruhe Institute of Technology, and the second workshop is held in Salzburg, Austria in February 2024. These workshops feature keynote lectures by experts, technical presentations, advanced tutorials, poster sessions, and contributions on RL applications in various facilities. The next upcoming workshop will be held in February 2023 at DESY, Hamburg.', 'One of the key metrics determining the capabilities of Free Electron Laser (FEL) facilities is the intensity of photon beam they can provide to experiments. However, in day-to-day operations, tuning to maximise the FEL intensity is one of the most difficult and time-consuming tasks. Skilled human operators still need large amounts of the available beam time, which are then not available for experiments, to achieve maximum performance. The large number of tuning parameters and high non-linearity of the underlying dynamics have so far made it challenging to develop autonomous FEL tuning solutions. We present a method based on reinforcement learning to train a neural network policy to autonomously tune the FEL intensity at *LCLS* and *European XFEL*. Our method is trained requiring little to no beam time and is appealing for tuning across different FEL setups. In contrast to conventional black box optimisation approaches that do not share information across different tuning sessions and setups, a trained policy can leverage its experience to tune the FEL intensity with minimal online exploration.', 'Online tuning of particle accelerators is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods like Bayesian optimisation (BO) hold great promise in improving plant performance and reducing tuning times. At the same time, Reinforcement Learning (RL) is a capable method of learning intelligent controllers, while recent work shows that RL can also be used to train domain-specialised optimisers in so-called Reinforcement Learning-      trained Optimisation (RLO). In parallel efforts, both algorithms have found successful adoption in particle accelerator tuning. Here we present a comparative case study, analysing the behaviours of both algorithms and outlining their strengths and weaknesses. The results of our study help provide criteria for choosing a suitable learning-based tuning algorithm for a given task and will accelerate research and adoption of these methods with particle accelerators and other complex real-      world facilities, ultimately improving their availability and pushing their operational limits, thereby enabling scientific and engineering advancements.']\n",
    "sentences = []\n",
    "sentences.extend(sentences1)\n",
    "sentences.extend(sentences2)\n",
    "print('ours')\n",
    "res = _eval(model1,sentences)\n",
    "res -= torch.diag(torch.diag(res))\n",
    "plt.imshow(res)\n",
    "plt.show()\n",
    "print('their')\n",
    "res = _eval(model2,sentences)\n",
    "res -= torch.diag(torch.diag(res))\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0fd17-773b-4b88-bcba-e13afb666726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c309af4-2f92-4c39-99c5-5abcf44da891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'BESSY',\n",
    "    'DESY',\n",
    "    'European XFEL',\n",
    "    'PETRA'\n",
    "    'HZB',\n",
    "    'synchroton',\n",
    "    'linac']\n",
    "print('ours')\n",
    "print(_eval(model1,sentences))\n",
    "print('their')\n",
    "print(_eval(model2,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d32d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
